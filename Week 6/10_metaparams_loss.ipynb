{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oMl3PkezSPHA"
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I_fNhEIyU8-5"
   },
   "source": [
    "# Mean-squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kww9GdqRS1_p"
   },
   "outputs": [],
   "source": [
    "# loss function\n",
    "lossfunMSE = nn.MSELoss()\n",
    "\n",
    "# create predictions and real answer\n",
    "yHat = torch.linspace(-2,2,101)\n",
    "y = torch.tensor(.5)\n",
    "\n",
    "# compute MSE loss function\n",
    "L = np.zeros(101)\n",
    "for i,yy in enumerate(yHat):\n",
    "  L[i] = lossfunMSE(yy,y)\n",
    "\n",
    "plt.plot(yHat,L,label='Loss')\n",
    "plt.plot([y,y],[0,np.max(L)],'r--',label='True value')\n",
    "plt.xlabel('Predicted value')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JW_RBi6wU65j"
   },
   "source": [
    "# Binary cross-entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HiN5ni12VEq0"
   },
   "outputs": [],
   "source": [
    "# loss function\n",
    "lossfunBCE = nn.BCELoss()\n",
    "# Define a binary cross-entropy loss function and store it in the variable 'lossfunBCE'.\n",
    "# BCELoss stands for Binary Cross-Entropy Loss, which is commonly used for binary classification problems.\n",
    "\n",
    "# create predictions and real answer\n",
    "yHat = torch.linspace(.001, .999, 101)\n",
    "# Create a range of predicted values 'yHat' ranging from 0.001 to 0.999 with 101 evenly spaced values.\n",
    "# These values represent the predicted probabilities of a binary classification problem.\n",
    "\n",
    "y1 = torch.tensor(0.)\n",
    "# Create a tensor 'y1' with a value of 0.0, representing the correct answer for the class label 0 (correct=0).\n",
    "# This is used to compute the loss when the correct answer is 0.\n",
    "\n",
    "y2 = torch.tensor(1.)\n",
    "# Create a tensor 'y2' with a value of 1.0, representing the correct answer for the class label 1 (correct=1).\n",
    "# This is used to compute the loss when the correct answer is 1.\n",
    "\n",
    "# compute BCE loss function\n",
    "L = np.zeros((101, 2))\n",
    "# Create an empty NumPy array 'L' with dimensions (101, 2) to store the computed loss values.\n",
    "# The first dimension represents the number of predicted values, and the second dimension represents two cases: correct=0 and correct=1.\n",
    "\n",
    "for i, yy in enumerate(yHat):\n",
    "    L[i, 0] = lossfunBCE(yy, y1)\n",
    "    # Compute the binary cross-entropy loss using 'lossfunBCE' for the case where the correct answer is 0.\n",
    "    # Store the computed loss in the 'L' array at position (i, 0).\n",
    "\n",
    "    L[i, 1] = lossfunBCE(yy, y2)\n",
    "    # Compute the binary cross-entropy loss using 'lossfunBCE' for the case where the correct answer is 1.\n",
    "    # Store the computed loss in the 'L' array at position (i, 1).\n",
    "\n",
    "plt.plot(yHat, L)\n",
    "# Create a plot of the predicted values on the x-axis and the computed losses on the y-axis.\n",
    "plt.xlabel('Predicted value')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['correct=0', 'correct=1'])\n",
    "# Add labels and a legend to the plot for clarity.\n",
    "# The legend indicates which line corresponds to each correct answer (0 or 1).\n",
    "plt.show()\n",
    "# Display the plot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J_SJFL9CBVGA"
   },
   "outputs": [],
   "source": [
    "# The example above shows data already in probabilities. Raw outputs will need to be converted to probabilities:\n",
    "\n",
    "# \"raw\" output of a model\n",
    "yHat = torch.tensor(2.)\n",
    "print(lossfunBCE(yHat,y2))\n",
    "\n",
    "# convert to prob via sigmoid\n",
    "sig = nn.Sigmoid()\n",
    "print(lossfunBCE( sig(yHat) ,y2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wQmVOg-iCLGB"
   },
   "outputs": [],
   "source": [
    "# However, PyTorch recommends using a single function that incorporates sigmoid+BCE due to increased numerical stability.\n",
    "# https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html?highlight=nn%20bcewithlogitsloss#torch.nn.BCEWithLogitsLoss\n",
    "\n",
    "# Thus, the recommended way to do it:\n",
    "lossfunBCE = nn.BCEWithLogitsLoss()\n",
    "# Define a binary cross-entropy loss function that incorporates sigmoid activation and logits.\n",
    "# This loss function is recommended by PyTorch for numerical stability.\n",
    "\n",
    "yHat = torch.tensor(2.)\n",
    "# Create a tensor 'yHat' with a single value of 2.0.\n",
    "# This represents the prediction value (logit) in a binary classification problem.\n",
    "\n",
    "print(lossfunBCE(yHat, y2))\n",
    "# Compute the binary cross-entropy loss using 'lossfunBCE' for the given prediction 'yHat' and the correct answer 'y2'.\n",
    "# Print the computed loss.\n",
    "\n",
    "# In toy examples, numerical accuracy usually isn't a problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TU_1uyOHWG7w"
   },
   "source": [
    "# Categorical cross-entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A868kRI3VEts"
   },
   "outputs": [],
   "source": [
    "# loss function\n",
    "lossfunCCE = nn.CrossEntropyLoss()\n",
    "# Define a cross-entropy loss function for multi-class classification problems.\n",
    "# This loss function is commonly used when dealing with classification tasks with multiple classes.\n",
    "\n",
    "# vector of output layer (pre-softmax)\n",
    "yHat = torch.tensor([[1., 4, 3]])\n",
    "# Create a tensor 'yHat' representing the output of the neural network's output layer before applying the softmax activation function.\n",
    "# In this example, 'yHat' contains a single row with three values, which could represent unnormalized scores for three classes.\n",
    "\n",
    "for i in range(3):\n",
    "    correctAnswer = torch.tensor([i])\n",
    "    # Create a tensor 'correctAnswer' representing the correct class label for the current iteration.\n",
    "    # The loop iterates over each possible correct answer (class).\n",
    "\n",
    "    thisloss = lossfunCCE(yHat, correctAnswer).item()\n",
    "    # Compute the cross-entropy loss using 'lossfunCCE' for the given 'yHat' and 'correctAnswer'.\n",
    "    # The '.item()' method retrieves the scalar value of the computed loss.\n",
    "\n",
    "    print('Loss when correct answer is %g: %g' % (i, thisloss))\n",
    "    # Print the computed loss along with the correct answer label for clarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f5sW84OScqNu"
   },
   "outputs": [],
   "source": [
    "# Repeat using pre-softmaxified output\n",
    "sm = nn.Softmax(dim=1)\n",
    "yHat_sm = sm(yHat)\n",
    "\n",
    "for i in range(3):\n",
    "  correctAnswer = torch.tensor([i])\n",
    "  thisloss = lossfunCCE(yHat_sm,correctAnswer).item()\n",
    "  print( 'Loss when correct answer is %g: %g' %(i,thisloss) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PskVgu-gK66H"
   },
   "outputs": [],
   "source": [
    "# compare raw, softmax, and log-softmax outputs\n",
    "sm = nn.LogSoftmax(dim=1)\n",
    "yHat_logsm = sm(yHat)\n",
    "\n",
    "# print them\n",
    "print(yHat)\n",
    "print(yHat_sm)\n",
    "print(yHat_logsm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qGtELAAfErbk"
   },
   "source": [
    "# Creating your own custom loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DlCcAbitHhpK"
   },
   "outputs": [],
   "source": [
    "class myLoss(nn.Module):\n",
    "    # Define a custom loss class 'myLoss' that inherits from nn.Module.\n",
    "    # This custom loss calculates the absolute difference between two values.\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Initialize the custom loss class and call the constructor of the parent class (nn.Module).\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # Define the forward method for the custom loss.\n",
    "        # It takes two input values 'x' and 'y' and computes the loss as the absolute difference between them.\n",
    "\n",
    "        loss = torch.abs(x - y)\n",
    "        # Calculate the absolute difference between 'x' and 'y' and store it in the variable 'loss'.\n",
    "\n",
    "        return loss\n",
    "        # Return the computed loss.\n",
    "\n",
    "# test it out!\n",
    "lfun = myLoss()\n",
    "# Create an instance of the custom loss class 'myLoss' and store it in the variable 'lfun'.\n",
    "\n",
    "lfun(torch.tensor(4), torch.tensor(5.2))\n",
    "# Use the 'lfun' instance to compute the loss by passing two tensors (4 and 5.2) as input.\n",
    "# The custom loss calculates the absolute difference between these values and returns the result.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM8nF/yQRb+pqekb1hGILaP",
   "collapsed_sections": [],
   "name": "DUDL_metaparams_loss.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
