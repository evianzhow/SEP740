{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j7-LiwqUMGYL"
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "display.set_matplotlib_formats('svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1IfH1Jiph-Uj"
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j-SP8NPsMNRL"
   },
   "outputs": [],
   "source": [
    "# create data\n",
    "\n",
    "nPerClust = 300\n",
    "blur = 1\n",
    "\n",
    "A = [ 1, 1 ]\n",
    "B = [ 5, 1 ]\n",
    "C = [ 4, 3 ]\n",
    "\n",
    "# generate data\n",
    "a = [ A[0]+np.random.randn(nPerClust)*blur , A[1]+np.random.randn(nPerClust)*blur ]\n",
    "b = [ B[0]+np.random.randn(nPerClust)*blur , B[1]+np.random.randn(nPerClust)*blur ]\n",
    "c = [ C[0]+np.random.randn(nPerClust)*blur , C[1]+np.random.randn(nPerClust)*blur ]\n",
    "\n",
    "# true labels\n",
    "labels_np = np.hstack((  np.zeros((nPerClust)),\n",
    "                         np.ones( (nPerClust)),\n",
    "                       1+np.ones( (nPerClust))  ))\n",
    "\n",
    "# concatanate into a matrix\n",
    "data_np = np.hstack((a,b,c)).T\n",
    "\n",
    "# convert to a pytorch tensor\n",
    "data = torch.tensor(data_np).float()\n",
    "labels = torch.tensor(labels_np).long() # note: \"long\" format for CCE\n",
    "\n",
    "# show the data\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "plt.plot(data[np.where(labels==0)[0],0],data[np.where(labels==0)[0],1],'bs',alpha=.5)\n",
    "plt.plot(data[np.where(labels==1)[0],0],data[np.where(labels==1)[0],1],'ko',alpha=.5)\n",
    "plt.plot(data[np.where(labels==2)[0],0],data[np.where(labels==2)[0],1],'r^',alpha=.5)\n",
    "plt.title('The qwerties!')\n",
    "plt.xlabel('qwerty dimension 1')\n",
    "plt.ylabel('qwerty dimension 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "05kSm4Jkjvd_"
   },
   "outputs": [],
   "source": [
    "# use scikitlearn to split the data\n",
    "train_data,test_data, train_labels,test_labels = train_test_split(data, labels, test_size=.1)\n",
    "\n",
    "# then convert them into PyTorch Datasets (note: already converted to tensors)\n",
    "train_data = TensorDataset(train_data,train_labels)\n",
    "test_data  = TensorDataset(test_data,test_labels)\n",
    "\n",
    "# finally, translate into dataloader objects\n",
    "batchsize    = 16\n",
    "train_loader = DataLoader(train_data,batch_size=batchsize,shuffle=True,drop_last=True)\n",
    "test_loader  = DataLoader(test_data,batch_size=test_data.tensors[0].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QUAMIe9WiAPw"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z0YpD6f-j8dG"
   },
   "outputs": [],
   "source": [
    "# Create a class for the neural network model and associated functions.\n",
    "\n",
    "# Define a class for the neural network model.\n",
    "def createTheQwertyNet(momentum):\n",
    "    # Define a custom class for the neural network model.\n",
    "    class qwertyNet(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "            # Input layer: It has 2 input features and connects to 8 neurons.\n",
    "            self.input = nn.Linear(2, 8)\n",
    "\n",
    "            # Hidden layer: One hidden layer with 8 neurons.\n",
    "            self.fc1 = nn.Linear(8, 8)\n",
    "\n",
    "            # Output layer: Connects to 3 neurons for classification (3 classes).\n",
    "            self.output = nn.Linear(8, 3)\n",
    "\n",
    "        # Forward pass: Defines how data flows through the network.\n",
    "        def forward(self, x):\n",
    "            # Apply Rectified Linear Unit (ReLU) activation to the input layer.\n",
    "            x = F.relu(self.input(x))\n",
    "            \n",
    "            # Apply ReLU activation to the hidden layer.\n",
    "            x = F.relu(self.fc1(x))\n",
    "            \n",
    "            # Pass the result through the output layer.\n",
    "            return self.output(x)\n",
    "\n",
    "    # Create an instance of the neural network model.\n",
    "    net = qwertyNet()\n",
    "\n",
    "    # Define the loss function (Cross Entropy Loss) for classification tasks.\n",
    "    lossfun = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Define the optimizer (Stochastic Gradient Descent) with specified learning rate and momentum.\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=0.01, momentum=momentum)\n",
    "\n",
    "    # Return the created neural network, loss function, and optimizer.\n",
    "    return net, lossfun, optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TE9tBBfycJ59"
   },
   "outputs": [],
   "source": [
    "# Confirm that the optimizer has momentum\n",
    "\n",
    "# Create an instance of the optimizer with specified momentum.\n",
    "optim = createTheQwertyNet(0.9)[2]\n",
    "\n",
    "# Output the optimizer instance.\n",
    "optim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upB4qhR0iCKv"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Q0nmoUPmu-5"
   },
   "outputs": [],
   "source": [
    "# a function that trains the model\n",
    "\n",
    "\n",
    "# number of epochs\n",
    "numepochs = 50\n",
    "\n",
    "\n",
    "def function2trainTheModel(momentum):\n",
    "  \n",
    "  # create a new model\n",
    "  net,lossfun,optimizer = createTheQwertyNet(momentum)\n",
    "\n",
    "  # initialize losses\n",
    "  losses   = torch.zeros(numepochs)\n",
    "  trainAcc = []\n",
    "  testAcc  = []\n",
    "\n",
    "  # loop over epochs\n",
    "  for epochi in range(numepochs):\n",
    "\n",
    "    # switch on training mode\n",
    "    net.train()\n",
    "\n",
    "    # loop over training data batches\n",
    "    batchAcc  = []\n",
    "    batchLoss = []\n",
    "    for X,y in train_loader:\n",
    "\n",
    "      # forward pass and loss\n",
    "      yHat = net(X)\n",
    "      loss = lossfun(yHat,y)\n",
    "\n",
    "      # backprop\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      # loss from this batch\n",
    "      batchLoss.append(loss.item())\n",
    "\n",
    "      # compute accuracy\n",
    "      matches = torch.argmax(yHat,axis=1) == y     # booleans (false/true)\n",
    "      matchesNumeric = matches.float()             # convert to numbers (0/1)\n",
    "      accuracyPct = 100*torch.mean(matchesNumeric) # average and x100 \n",
    "      batchAcc.append( accuracyPct )               # add to list of accuracies\n",
    "    # end of batch loop...\n",
    "\n",
    "    # now that we've trained through the batches, get their average training accuracy\n",
    "    trainAcc.append( np.mean(batchAcc) )\n",
    "\n",
    "    # and get average losses across the batches\n",
    "    losses[epochi] = np.mean(batchLoss)\n",
    "\n",
    "    # test accuracy\n",
    "    net.eval()\n",
    "    X,y = next(iter(test_loader)) # extract X,y from test dataloader\n",
    "    with torch.no_grad(): # deactivates autograd\n",
    "      yHat = net(X)\n",
    "      \n",
    "    # compare the following really long line of code to the training accuracy lines\n",
    "    testAcc.append( 100*torch.mean((torch.argmax(yHat,axis=1)==y).float()) ) \n",
    "  # end epochs\n",
    "\n",
    "  # function output\n",
    "  return trainAcc,testAcc,losses,net\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NYA0_iEVZOS2"
   },
   "source": [
    "# Now for the experiment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tMCInlW6xKCw"
   },
   "outputs": [],
   "source": [
    "# Test different momentum values during training\n",
    "\n",
    "# Explanation:\n",
    "# In this code snippet, we are conducting an experiment to evaluate the impact of different momentum values during the training of a neural network model.\n",
    "# The `momenta` list contains various momentum values (0, 0.5, 0.9, 0.95, 0.999) that we want to test.\n",
    "\n",
    "# Initialize a results matrix to store training results.\n",
    "results = np.zeros((numepochs, len(momenta), 3))\n",
    "\n",
    "# Explanation:\n",
    "# The `results` matrix is a multidimensional array with dimensions (numepochs, len(momenta), 3). It will be used to store training results, including losses, training accuracy, and test accuracy. Each dimension of the matrix corresponds to different aspects of the results.\n",
    "\n",
    "# Test all momentum values on the same data with different model instances.\n",
    "for idx, mom in enumerate(momenta):\n",
    "    # Explanation:\n",
    "    # Within this loop, we iterate through the `momenta` list, and for each momentum value (`mom`), we call a function (presumably `function2trainTheModel`) to train the neural network model with that specific momentum value.\n",
    "\n",
    "    # Call a function to train the model with the specified momentum.\n",
    "    trainAcc, testAcc, losses, net = function2trainTheModel(mom)\n",
    "    \n",
    "    # Explanation:\n",
    "    # The training results, including training accuracy, test accuracy, and losses, are obtained from the training process using the specified momentum value. These results are then stored in the `results` matrix.\n",
    "\n",
    "    # Store the training losses, training accuracy, and test accuracy in the results matrix.\n",
    "    results[:, idx, 0] = losses  # Column 0 stores losses\n",
    "    results[:, idx, 1] = trainAcc  # Column 1 stores training accuracy\n",
    "    results[:, idx, 2] = testAcc  # Column 2 stores test accuracy\n",
    "\n",
    "# Explanation:\n",
    "# After the loop completes, the `results` matrix contains the training results for each tested momentum value. This experimentation helps us understand how different momentum values affect the training process of the neural network model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_uB4c8mAWCx-"
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,3,figsize=(16,5))\n",
    "\n",
    "for i in range(3):\n",
    "  ax[i].plot(results[:,:,i])\n",
    "  ax[i].legend(momenta)\n",
    "  ax[i].set_xlabel('Epochs')\n",
    "  if i==0:\n",
    "    ax[i].set_ylabel('Loss')\n",
    "  else:\n",
    "    ax[i].set_ylabel('Accuracy (%)')\n",
    "    ax[i].set_ylim([20,100])\n",
    "\n",
    "ax[0].set_title('Losses')\n",
    "ax[1].set_title('Train')\n",
    "ax[2].set_title('Test')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GRZjAJImZQvW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8UaYYlsRZR-h"
   },
   "source": [
    "# Additional explorations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2_NUa-ZTZTy4"
   },
   "outputs": [],
   "source": [
    "# 1) Now that you see the results across a broad range of beta (momentum) parameters, try re-running the experiment\n",
    "#    using a narrower range. For example, you don't need to test b=0 or b=.999.\n",
    "# \n",
    "# 2) The beta parameter multiplies the learning rate in the computation (see formula in slides). That means that these\n",
    "#    results will interact with the learning rate. Repeat the experiment using a different learning rate.\n",
    "# \n",
    "# 3) If you wanted to test the relationship between momentum and learning rate in a full parametric experiment, how would\n",
    "#    you set it up? Would you store the loss/accuracy over all epochs? \n",
    "# "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMXgjACK639xApm/vmOxTzq",
   "provenance": [
    {
     "file_id": "1Yp9bgltmsXuxkNPmbKEC1kn7bkBQD5WD",
     "timestamp": 1617649131489
    },
    {
     "file_id": "10_geQnah5AvMsm8VDAQwNPhypOXradar",
     "timestamp": 1617634658608
    },
    {
     "file_id": "1FtQ99beHYcDFDywLdaPgFm-KjBeI8PvD",
     "timestamp": 1615877547147
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
