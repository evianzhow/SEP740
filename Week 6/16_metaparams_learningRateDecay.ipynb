{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j7-LiwqUMGYL"
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "display.set_matplotlib_formats('svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xu7HL2dFi87O"
   },
   "source": [
    "# The data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j-SP8NPsMNRL"
   },
   "outputs": [],
   "source": [
    "# create data\n",
    "\n",
    "nPerClust = 300\n",
    "blur = 1\n",
    "\n",
    "A = [ 1, 1 ]\n",
    "B = [ 5, 1 ]\n",
    "C = [ 4, 3 ]\n",
    "\n",
    "# generate data\n",
    "a = [ A[0]+np.random.randn(nPerClust)*blur , A[1]+np.random.randn(nPerClust)*blur ]\n",
    "b = [ B[0]+np.random.randn(nPerClust)*blur , B[1]+np.random.randn(nPerClust)*blur ]\n",
    "c = [ C[0]+np.random.randn(nPerClust)*blur , C[1]+np.random.randn(nPerClust)*blur ]\n",
    "\n",
    "# true labels\n",
    "labels_np = np.hstack((  np.zeros((nPerClust)),\n",
    "                         np.ones( (nPerClust)),\n",
    "                       1+np.ones( (nPerClust))  ))\n",
    "\n",
    "# concatanate into a matrix\n",
    "data_np = np.hstack((a,b,c)).T\n",
    "\n",
    "# convert to a pytorch tensor\n",
    "data = torch.tensor(data_np).float()\n",
    "labels = torch.tensor(labels_np).long() # note: \"long\" format for CCE\n",
    "\n",
    "# show the data\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "plt.plot(data[np.where(labels==0)[0],0],data[np.where(labels==0)[0],1],'bs',alpha=.5)\n",
    "plt.plot(data[np.where(labels==1)[0],0],data[np.where(labels==1)[0],1],'ko',alpha=.5)\n",
    "plt.plot(data[np.where(labels==2)[0],0],data[np.where(labels==2)[0],1],'r^',alpha=.5)\n",
    "plt.title('The qwerties!')\n",
    "plt.xlabel('qwerty dimension 1')\n",
    "plt.ylabel('qwerty dimension 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "05kSm4Jkjvd_"
   },
   "outputs": [],
   "source": [
    "# use scikitlearn to split the data\n",
    "train_data,test_data, train_labels,test_labels = train_test_split(data, labels, test_size=.1)\n",
    "\n",
    "# then convert them into PyTorch Datasets (note: already converted to tensors)\n",
    "train_data = TensorDataset(train_data,train_labels)\n",
    "test_data  = TensorDataset(test_data,test_labels)\n",
    "\n",
    "# finally, translate into dataloader objects\n",
    "batchsize    = 16\n",
    "train_loader = DataLoader(train_data,batch_size=batchsize,shuffle=True,drop_last=True)\n",
    "test_loader  = DataLoader(test_data,batch_size=test_data.tensors[0].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "187Sw2tBi_V1"
   },
   "source": [
    "# The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z0YpD6f-j8dG"
   },
   "outputs": [],
   "source": [
    "# create a class for the model\n",
    "def createTheQwertyNet(initialLR):\n",
    "\n",
    "  class qwertyNet(nn.Module):\n",
    "    def __init__(self):\n",
    "      super().__init__()\n",
    "\n",
    "      ### input layer\n",
    "      self.input = nn.Linear(2,8)\n",
    "      \n",
    "      ### hidden layer\n",
    "      self.fc1 = nn.Linear(8,8)\n",
    "\n",
    "      ### output layer\n",
    "      self.output = nn.Linear(8,3)\n",
    "\n",
    "    # forward pass\n",
    "    def forward(self,x):\n",
    "      x = F.relu( self.input(x) )\n",
    "      x = F.relu( self.fc1(x) )\n",
    "      return self.output(x)\n",
    "  \n",
    "  # create the model instance\n",
    "  net = qwertyNet()\n",
    "  \n",
    "  # loss function\n",
    "  lossfun = nn.CrossEntropyLoss()\n",
    "\n",
    "  # optimizer and LR scheduler\n",
    "  optimizer = torch.optim.SGD(net.parameters(),lr=initialLR)\n",
    "  stepsize  = batchsize*len(train_loader)\n",
    "  scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size=stepsize,gamma=.5)\n",
    "\n",
    "  return net,lossfun,optimizer,scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "70KXvLB65D7l"
   },
   "outputs": [],
   "source": [
    "# Calculate the number of steps until the learning rate changes\n",
    "steps_until_change = len(train_loader) * batchsize\n",
    "\n",
    "# Explanation:\n",
    "# We calculate the number of steps or iterations that will be performed in the training process until the learning rate changes. This calculation is based on the length of the 'train_loader,' which represents the training data divided into batches, and 'batchsize,' which specifies the size of each batch.\n",
    "\n",
    "# For example, if there are 100 batches in the 'train_loader' and each batch contains 16 samples (as specified by 'batchsize'), then there will be a total of 100 * 16 = 1600 steps or iterations until the learning rate changes. This value indicates how many updates the model parameters will undergo before the learning rate scheduler takes effect and adjusts the learning rate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ww_grqcEjBZ9"
   },
   "source": [
    "# Explore the learning rate decay parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RhBtCQultc-8"
   },
   "outputs": [],
   "source": [
    "# Create a network\n",
    "net = createTheQwertyNet(0.01)[0]\n",
    "\n",
    "# Explanation:\n",
    "# We create a neural network model called 'net' using the 'createTheQwertyNet' function with a specified L2 regularization strength of 0.01. The '[0]' at the end of the function call indicates that we are interested in the model itself, not the loss function or optimizer.\n",
    "\n",
    "# Create a new optimizer\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=1/2)\n",
    "\n",
    "# Explanation:\n",
    "# We define a new optimizer for our neural network model. In this case, we use Stochastic Gradient Descent (SGD) as the optimization algorithm with a learning rate of 0.01. Additionally, we create a learning rate scheduler called 'scheduler' using the 'torch.optim.lr_scheduler.StepLR' class. The scheduler reduces the learning rate during training to improve convergence. Here, it is configured to decrease the learning rate by half every 5 epochs.\n",
    "\n",
    "# Test the change in learning rate\n",
    "for epoch in range(3):\n",
    "  for batchnum in range(10):\n",
    "    print(f'Batch {batchnum}, epoch {epoch}: LR={scheduler.get_last_lr()[0]}')\n",
    "    scheduler.step()\n",
    "\n",
    "# Explanation:\n",
    "# We enter a loop where we iterate through training epochs and batches within each epoch.\n",
    "\n",
    "# - For each epoch (in this case, we have 3 epochs), we loop through 10 batches (specified by 'range(10)').\n",
    "\n",
    "# - Inside the batch loop, we print the batch number, epoch number, and the current learning rate using the 'scheduler.get_last_lr()' function. This function retrieves the most recent learning rate value determined by the scheduler.\n",
    "\n",
    "# - After printing the information, we call 'scheduler.step()' to update the learning rate according to the scheduler's policy.\n",
    "\n",
    "# The output shows how the learning rate changes over batches and epochs based on the scheduler's configuration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FUPxZERCjLa9"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Q0nmoUPmu-5"
   },
   "outputs": [],
   "source": [
    "# a function that trains the model\n",
    "\n",
    "def function2trainTheModel(initialLR,toggleDynamicLR):\n",
    "\n",
    "  # number of epochs\n",
    "  numepochs = 50\n",
    "  \n",
    "  # create a new model\n",
    "  net,lossfun,optimizer,scheduler = createTheQwertyNet(initialLR)\n",
    "\n",
    "  # initialize losses\n",
    "  losses    = torch.zeros(numepochs)\n",
    "  trainAcc  = []\n",
    "  testAcc   = []\n",
    "  currentLR = []\n",
    "\n",
    "\n",
    "  # loop over epochs\n",
    "  for epochi in range(numepochs):\n",
    "\n",
    "    # switch on training mode\n",
    "    net.train()\n",
    "\n",
    "    # loop over training data batches\n",
    "    batchAcc  = []\n",
    "    batchLoss = []\n",
    "    for X,y in train_loader:\n",
    "\n",
    "      # forward pass and loss\n",
    "      yHat = net(X)\n",
    "      loss = lossfun(yHat,y)\n",
    "\n",
    "      # backprop\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      # step the learning-rate scheduler\n",
    "      if toggleDynamicLR:\n",
    "        scheduler.step()\n",
    "\n",
    "      # loss from this batch\n",
    "      batchLoss.append(loss.item())\n",
    "\n",
    "      # compute accuracy\n",
    "      matches = torch.argmax(yHat,axis=1) == y     # booleans (false/true)\n",
    "      matchesNumeric = matches.float()             # convert to numbers (0/1)\n",
    "      accuracyPct = 100*torch.mean(matchesNumeric) # average and x100 \n",
    "      batchAcc.append( accuracyPct )               # add to list of accuracies\n",
    "\n",
    "      currentLR.append( scheduler.get_last_lr()[0] )\n",
    "    # end of batch loop...\n",
    "\n",
    "    # now that we've trained through the batches, get their average training accuracy\n",
    "    trainAcc.append( np.mean(batchAcc) )\n",
    "\n",
    "    # and get average losses across the batches\n",
    "    losses[epochi] = np.mean(batchLoss)\n",
    "\n",
    "    # test accuracy\n",
    "    net.eval()\n",
    "    X,y = next(iter(test_loader)) # extract X,y from test dataloader\n",
    "    with torch.no_grad(): # deactivates autograd\n",
    "      yHat = net(X)\n",
    "      \n",
    "    # compare the following really long line of code to the training accuracy lines\n",
    "    testAcc.append( 100*torch.mean((torch.argmax(yHat,axis=1)==y).float()) )\n",
    "\n",
    "  # end epochs\n",
    "\n",
    "  # function output\n",
    "  return trainAcc,testAcc,losses,net,currentLR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b6IvFge5jSyd"
   },
   "source": [
    "# Test that the model really changes (sanity checks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rM8megPCxl6z"
   },
   "outputs": [],
   "source": [
    "# test that the learning rate is really working\n",
    "trainAcc,testAcc,losses,net,currentLR = function2trainTheModel(.01,True)\n",
    "plt.plot(currentLR)\n",
    "plt.title('Learning rate should change')\n",
    "plt.show()\n",
    "\n",
    "trainAcc,testAcc,losses,net,currentLR = function2trainTheModel(.01,False)\n",
    "plt.plot(currentLR)\n",
    "plt.title('Learning rate should stay fixed')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9aNhB3LOjflJ"
   },
   "source": [
    "# Now for the experiment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "swOdd4Z1yFXa"
   },
   "outputs": [],
   "source": [
    "# now test with and without dynamic LR\n",
    "trainAccDynamic,testAccDynamic,losses,net,currentLR = function2trainTheModel(.01,True)\n",
    "trainAccStatic,testAccStatic,losses,net,currentLR   = function2trainTheModel(.01,False)\n",
    "\n",
    "fig = plt.figure(figsize=(8,5))\n",
    "plt.plot(trainAccDynamic,'r',label='Dyn: Train')\n",
    "plt.plot(testAccDynamic,'r--',label='Dyn: Test')\n",
    "\n",
    "plt.plot(trainAccStatic,'b',label='Stat: Train')\n",
    "plt.plot(testAccStatic,'b--',label='Stat: Test')\n",
    "\n",
    "plt.xlabel('Training epochs')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Accuracy over epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D_IHeTesm8FV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aDwJ_UP4m8VO"
   },
   "source": [
    "# Additional explorations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8lvhTu3-m925"
   },
   "outputs": [],
   "source": [
    "# 1) When you run the experiment in the previous cell multiple times, you can get different answers. This indicates\n",
    "#    that the network and/or training regimen is not stable enough. What can you do to increase the stability of the\n",
    "#    model and training? That is, what can you change to make the results more similar each time you re-run the experiment?\n",
    "# \n",
    "# 2) There are several more options for dynamic learning rates in Pytorch. Try modifying the code!\n",
    "#       See https://pytorch.org/docs/stable/optim.html\n",
    "# "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNPp1XFOaWomCYumdrW/sm+",
   "provenance": [
    {
     "file_id": "1-oU0PNeBHJGzsk7YbKtllYMeKibAQ51s",
     "timestamp": 1617653700897
    },
    {
     "file_id": "1M7eY1x1xa6KsvY0Q6w5RxLXFHc0kbuJQ",
     "timestamp": 1617651927554
    },
    {
     "file_id": "1Yp9bgltmsXuxkNPmbKEC1kn7bkBQD5WD",
     "timestamp": 1617649131489
    },
    {
     "file_id": "10_geQnah5AvMsm8VDAQwNPhypOXradar",
     "timestamp": 1617634658608
    },
    {
     "file_id": "1FtQ99beHYcDFDywLdaPgFm-KjBeI8PvD",
     "timestamp": 1615877547147
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
