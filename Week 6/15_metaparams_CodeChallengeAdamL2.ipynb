{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j7-LiwqUMGYL"
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "display.set_matplotlib_formats('svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j-SP8NPsMNRL"
   },
   "outputs": [],
   "source": [
    "# create data\n",
    "\n",
    "nPerClust = 300\n",
    "blur = 1\n",
    "\n",
    "A = [ 1, 1 ]\n",
    "B = [ 5, 1 ]\n",
    "C = [ 4, 3 ]\n",
    "\n",
    "# generate data\n",
    "a = [ A[0]+np.random.randn(nPerClust)*blur , A[1]+np.random.randn(nPerClust)*blur ]\n",
    "b = [ B[0]+np.random.randn(nPerClust)*blur , B[1]+np.random.randn(nPerClust)*blur ]\n",
    "c = [ C[0]+np.random.randn(nPerClust)*blur , C[1]+np.random.randn(nPerClust)*blur ]\n",
    "\n",
    "# true labels\n",
    "labels_np = np.hstack((  np.zeros((nPerClust)),\n",
    "                         np.ones( (nPerClust)),\n",
    "                       1+np.ones( (nPerClust))  ))\n",
    "\n",
    "# concatanate into a matrix\n",
    "data_np = np.hstack((a,b,c)).T\n",
    "\n",
    "# convert to a pytorch tensor\n",
    "data = torch.tensor(data_np).float()\n",
    "labels = torch.tensor(labels_np).long() # note: \"long\" format for CCE\n",
    "\n",
    "# show the data\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "plt.plot(data[np.where(labels==0)[0],0],data[np.where(labels==0)[0],1],'bs',alpha=.5)\n",
    "plt.plot(data[np.where(labels==1)[0],0],data[np.where(labels==1)[0],1],'ko',alpha=.5)\n",
    "plt.plot(data[np.where(labels==2)[0],0],data[np.where(labels==2)[0],1],'r^',alpha=.5)\n",
    "plt.title('The qwerties!')\n",
    "plt.xlabel('qwerty dimension 1')\n",
    "plt.ylabel('qwerty dimension 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "05kSm4Jkjvd_"
   },
   "outputs": [],
   "source": [
    "# use scikitlearn to split the data\n",
    "train_data,test_data, train_labels,test_labels = train_test_split(data, labels, test_size=.1)\n",
    "\n",
    "# then convert them into PyTorch Datasets (note: already converted to tensors)\n",
    "train_data = torch.utils.data.TensorDataset(train_data,train_labels)\n",
    "test_data  = torch.utils.data.TensorDataset(test_data,test_labels)\n",
    "\n",
    "# finally, translate into dataloader objects\n",
    "batchsize    = 32\n",
    "train_loader = DataLoader(train_data,batch_size=batchsize,shuffle=True,drop_last=True)\n",
    "test_loader  = DataLoader(test_data,batch_size=test_data.tensors[0].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z0YpD6f-j8dG"
   },
   "outputs": [],
   "source": [
    "# Create a class for the model\n",
    "def createTheQwertyNet(L2lambda):\n",
    "\n",
    "  # Explanation:\n",
    "  # In this code snippet, we define a neural network model class called 'qwertyNet.' This class represents a feedforward neural network with input, hidden, and output layers.\n",
    "\n",
    "  class qwertyNet(nn.Module):\n",
    "    def __init__(self):\n",
    "      super().__init__()\n",
    "\n",
    "      ### Input layer\n",
    "      self.input = nn.Linear(2, 8)\n",
    "      \n",
    "      ### Hidden layer\n",
    "      self.fc1 = nn.Linear(8, 8)\n",
    "\n",
    "      ### Output layer\n",
    "      self.output = nn.Linear(8, 3)\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, x):\n",
    "      x = F.relu(self.input(x))\n",
    "      x = F.relu(self.fc1(x))\n",
    "      return self.output(x)\n",
    "  \n",
    "  # Create the model instance\n",
    "  net = qwertyNet()\n",
    "\n",
    "  # Explanation:\n",
    "  # We create an instance of the 'qwertyNet' neural network model.\n",
    "\n",
    "  # Loss function\n",
    "  lossfun = nn.CrossEntropyLoss()\n",
    "\n",
    "  # Explanation:\n",
    "  # We define the loss function for the model. In this case, 'nn.CrossEntropyLoss' is used, which is commonly used for classification problems.\n",
    "\n",
    "  # Optimizer\n",
    "  optimizer = torch.optim.Adam(net.parameters(), lr=0.001, weight_decay=L2lambda)\n",
    "\n",
    "  # Explanation:\n",
    "  # We configure the optimizer for training the model. Here, we use the Adam optimizer ('torch.optim.Adam') with a learning rate of 0.001 and L2 regularization specified by 'weight_decay=L2lambda.'\n",
    "\n",
    "  return net, lossfun, optimizer\n",
    "\n",
    "# Explanation:\n",
    "# The 'createTheQwertyNet' function creates and configures a neural network model along with its loss function and optimizer. The 'L2lambda' parameter is used to control the strength of L2 regularization applied during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Q0nmoUPmu-5"
   },
   "outputs": [],
   "source": [
    "# a function that trains the model\n",
    "\n",
    "def function2trainTheModel(L2lambda):\n",
    "\n",
    "  # create a new model\n",
    "  net,lossfun,optimizer = createTheQwertyNet(L2lambda)\n",
    "\n",
    "  # initialize losses\n",
    "  losses   = torch.zeros(numepochs)\n",
    "  trainAcc = []\n",
    "  testAcc  = []\n",
    "\n",
    "  # loop over epochs\n",
    "  for epochi in range(numepochs):\n",
    "\n",
    "    # switch on training mode\n",
    "    net.train()\n",
    "\n",
    "    # loop over training data batches\n",
    "    batchAcc  = []\n",
    "    batchLoss = []\n",
    "    for X,y in train_loader:\n",
    "\n",
    "      # forward pass and loss\n",
    "      yHat = net(X)\n",
    "      loss = lossfun(yHat,y)\n",
    "\n",
    "      # backprop\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      # loss from this batch\n",
    "      batchLoss.append(loss.item())\n",
    "\n",
    "      # compute accuracy\n",
    "      matches = torch.argmax(yHat,axis=1) == y     # booleans (false/true)\n",
    "      matchesNumeric = matches.float()             # convert to numbers (0/1)\n",
    "      accuracyPct = 100*torch.mean(matchesNumeric) # average and x100 \n",
    "      batchAcc.append( accuracyPct )               # add to list of accuracies\n",
    "    # end of batch loop...\n",
    "\n",
    "    # now that we've trained through the batches, get their average training accuracy\n",
    "    trainAcc.append( np.mean(batchAcc) )\n",
    "\n",
    "    # and get average losses across the batches\n",
    "    losses[epochi] = np.mean(batchLoss)\n",
    "\n",
    "    # test accuracy\n",
    "    net.eval()\n",
    "    X,y = next(iter(test_loader)) # extract X,y from test dataloader\n",
    "    with torch.no_grad(): # deactivates autograd\n",
    "      yHat = net(X)\n",
    "      \n",
    "    # compare the following really long line of code to the training accuracy lines\n",
    "    testAcc.append( 100*torch.mean((torch.argmax(yHat,axis=1)==y).float()) ) \n",
    "  # end epochs\n",
    "\n",
    "  # function output\n",
    "  return trainAcc,testAcc,losses,net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tMCInlW6xKCw"
   },
   "outputs": [],
   "source": [
    "# Range of L2 regularization amounts\n",
    "l2lambdas = np.linspace(0, 0.1, 6)\n",
    "\n",
    "# Explanation:\n",
    "# We define a range of L2 regularization strengths (lambda values) to be tested. L2 regularization is used to prevent overfitting during training. The 'l2lambdas' array contains six values ranging from 0 to 0.1.\n",
    "\n",
    "# Number of training epochs\n",
    "numepochs = 50\n",
    "\n",
    "# Explanation:\n",
    "# We specify the number of training epochs. Each epoch represents a complete pass through the training dataset during training.\n",
    "\n",
    "# Initialize output results matrices\n",
    "accuracyResultsTrain = np.zeros((numepochs, len(l2lambdas)))\n",
    "accuracyResultsTest = np.zeros((numepochs, len(l2lambdas)))\n",
    "\n",
    "# Explanation:\n",
    "# We initialize two matrices to store the training and test accuracy results over multiple epochs and for different L2 regularization strengths. 'accuracyResultsTrain' will store training accuracy, and 'accuracyResultsTest' will store test accuracy.\n",
    "\n",
    "# Loop over L2 regularization strengths\n",
    "for li in range(len(l2lambdas)):\n",
    "\n",
    "  # Create and train a model\n",
    "  trainAcc, testAcc, losses, net = function2trainTheModel(l2lambdas[li])\n",
    "\n",
    "  # Explanation:\n",
    "  # We loop through each L2 regularization strength ('l2lambdas[li]') and perform the following steps for each strength.\n",
    "\n",
    "  # Create and train a model:\n",
    "  # The 'function2trainTheModel' function is called to create and train a neural network model with the specified L2 regularization strength. This function likely configures the model architecture, loss function, optimizer, and performs training.\n",
    "\n",
    "  # Store data:\n",
    "  # The training and test accuracy values obtained during training are stored in the 'accuracyResultsTrain' and 'accuracyResultsTest' matrices for the current L2 regularization strength.\n",
    "\n",
    "# Explanation:\n",
    "# After the loop completes, 'accuracyResultsTrain' contains the training accuracy values over multiple epochs for different L2 regularization strengths, and 'accuracyResultsTest' contains the corresponding test accuracy values. These results can be analyzed to understand how different levels of L2 regularization impact model performance during training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pIMi3DxQlgnK"
   },
   "outputs": [],
   "source": [
    "# plot some results\n",
    "fig,ax = plt.subplots(1,2,figsize=(15,5))\n",
    "\n",
    "ax[0].plot(accuracyResultsTrain,linewidth=2)\n",
    "ax[0].set_title('Train accuracy')\n",
    "ax[1].plot(accuracyResultsTest,linewidth=2)\n",
    "ax[1].set_title('Test accuracy')\n",
    "\n",
    "# make the legend easier to read\n",
    "leglabels = [np.round(i,2) for i in l2lambdas]\n",
    "\n",
    "# common features\n",
    "for i in range(2):\n",
    "  ax[i].legend(leglabels)\n",
    "  ax[i].set_xlabel('Epoch')\n",
    "  ax[i].set_ylabel('Accuracy (%)')\n",
    "  ax[i].set_ylim([30,101])\n",
    "  ax[i].grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mu66-60ig5iV"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO+bFYsv8RTHNPaW0/VwuVi",
   "provenance": [
    {
     "file_id": "1-oU0PNeBHJGzsk7YbKtllYMeKibAQ51s",
     "timestamp": 1617700421345
    },
    {
     "file_id": "1M7eY1x1xa6KsvY0Q6w5RxLXFHc0kbuJQ",
     "timestamp": 1617651927554
    },
    {
     "file_id": "1Yp9bgltmsXuxkNPmbKEC1kn7bkBQD5WD",
     "timestamp": 1617649131489
    },
    {
     "file_id": "10_geQnah5AvMsm8VDAQwNPhypOXradar",
     "timestamp": 1617634658608
    },
    {
     "file_id": "1FtQ99beHYcDFDywLdaPgFm-KjBeI8PvD",
     "timestamp": 1615877547147
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
